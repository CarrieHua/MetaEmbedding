{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:48.367428Z",
     "start_time": "2019-06-15T08:01:45.904265Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panfy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import Dense\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:48.374948Z",
     "start_time": "2019-06-15T08:01:48.370777Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Config\n",
    "'''\n",
    "# batch size per iteration\n",
    "BATCHSIZE = 200\n",
    "# mini-batch size for few-shot learning\n",
    "MINIBATCHSIZE = 20 \n",
    "# learning rate\n",
    "LR = 1e-3 \n",
    "# coefficient to balance `cold-start' and `warm-up'\n",
    "ALPHA = 0.1\n",
    "# length of embedding vectors\n",
    "EMB_SIZE = 256\n",
    "# model name\n",
    "MODEL = \"FM\"\n",
    "# log file\n",
    "LOG = \"logs/{}.csv\".format(MODEL)\n",
    "# path to save the model\n",
    "saver_path =\"saver/model\"+LOG.split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:48.385244Z",
     "start_time": "2019-06-15T08:01:48.377327Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        t = pickle.load(f)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:55.468974Z",
     "start_time": "2019-06-15T08:01:48.388318Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data of big ads\n",
    "train = read_pkl(\"../data/big_train_main.pkl\")\n",
    "# define the feature columns\n",
    "num_words_dict = {\n",
    "    'MovieID': 4000,\n",
    "    'UserID': 6050,\n",
    "    'Age': 7,\n",
    "    'Gender': 2,\n",
    "    'Occupation': 21,\n",
    "    'Year': 83,\n",
    "    \"Title\": 20001,\n",
    "    \"Genres\": 21,\n",
    "}\n",
    "ID_col = 'MovieID'\n",
    "item_col = ['Year', 'Title', 'Genres']\n",
    "context_col = ['Age', 'Gender', 'Occupation', 'UserID']\n",
    "cols = [ID_col]+item_col+context_col\n",
    "maxlen_dict = {\"Title\": 8, \"Genres\": 4}\n",
    "\n",
    "train_y = train['y'].values\n",
    "train_x = train[cols].to_dict('l')\n",
    "for col in maxlen_dict:\n",
    "    train_x[col] = pad_sequences(train[col], maxlen=maxlen_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:57.374178Z",
     "start_time": "2019-06-15T08:01:55.471668Z"
    }
   },
   "outputs": [],
   "source": [
    "# few-shot data\n",
    "test_a = read_pkl(\"../data/test_oneshot_a.pkl\")\n",
    "test_b = read_pkl(\"../data/test_oneshot_b.pkl\")\n",
    "test_c = read_pkl(\"../data/test_oneshot_c.pkl\")\n",
    "test_test = read_pkl(\"../data/test_test.pkl\")\n",
    "\n",
    "test_x_a = test_a[cols].to_dict('l')\n",
    "test_y_a = test_a['y'].values\n",
    "for col in maxlen_dict:\n",
    "    test_x_a[col] = pad_sequences(test_a[col], maxlen=maxlen_dict[col])\n",
    "\n",
    "test_x_b = test_b[cols].to_dict('l')\n",
    "test_y_b = test_b['y'].values\n",
    "for col in maxlen_dict:\n",
    "    test_x_b[col] = pad_sequences(test_b[col], maxlen=maxlen_dict[col])\n",
    "\n",
    "test_x_c = test_c[cols].to_dict('l')\n",
    "test_y_c = test_c['y'].values\n",
    "for col in maxlen_dict:\n",
    "    test_x_c[col] = pad_sequences(test_c[col], maxlen=maxlen_dict[col])\n",
    "\n",
    "test_x_test = test_test[cols].to_dict('l')\n",
    "test_y_test = test_test['y'].values\n",
    "for col in maxlen_dict:\n",
    "    test_x_test[col] = pad_sequences(test_test[col], maxlen=maxlen_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:01:57.433350Z",
     "start_time": "2019-06-15T08:01:57.377264Z"
    },
    "code_folding": [
     64,
     73,
     83,
     85,
     93,
     100,
     106,
     118,
     131
    ]
   },
   "outputs": [],
   "source": [
    "class Meta_Model(object):\n",
    "    def __init__(self, ID_col, item_col, context_col, nb_words, maxlen_dict={},\n",
    "                 model_name='FM', \n",
    "                 emb_size=128, alpha=0.2,\n",
    "                 warm_lr=1e-3, cold_lr=1e-4, ME_lr=1e-3):\n",
    "        \"\"\"\n",
    "        ID_col: string, the column name of the item ID\n",
    "        item_col: list, the columns of item features\n",
    "        context_col: list, the columns of other features\n",
    "        nb_words: dict, nb of words in each of these columns\n",
    "        get_yhat: the base model, default DeepFM\n",
    "        \"\"\"\n",
    "        columns = [ID_col] + item_col + context_col\n",
    "        def get_embeddings():\n",
    "            inputs, tables = {}, []\n",
    "            item_embs, other_embs, lr_weights = [], [], []\n",
    "            for col in columns:\n",
    "                if col not in maxlen_dict:\n",
    "                    inputs[col] = tf.placeholder(tf.int32, [None], name='input_'+col)\n",
    "                    table = tf.get_variable(\n",
    "                        \"table_{}\".format(col), [nb_words[col], emb_size],\n",
    "                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                    lr_table = tf.get_variable(\n",
    "                        \"lrtable_{}\".format(col), [nb_words[col], 1])\n",
    "                    emb = tf.nn.embedding_lookup(table, inputs[col])\n",
    "                    lr_w = tf.nn.embedding_lookup(lr_table, inputs[col])\n",
    "                else:\n",
    "                    inputs[col] = tf.placeholder(tf.int32, [None, maxlen_dict[col]], name='input_'+col)\n",
    "                    table = tf.get_variable(\n",
    "                        \"table_{}\".format(col), [nb_words[col], emb_size],\n",
    "                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                    lr_table = tf.get_variable(\n",
    "                        \"lrtable_{}\".format(col), [nb_words[col], 1])\n",
    "                    emb = tf.reduce_mean(tf.contrib.layers.embed_sequence(inputs[col], nb_words[col], emb_size, scope=col), axis=1)\n",
    "                    lr_w = tf.reduce_mean(emb, axis=-1, keepdims=True)\n",
    "                if col==ID_col:\n",
    "                    ID_emb = emb\n",
    "                    ID_table = table\n",
    "                elif col in item_col:\n",
    "                    item_embs.append(emb)\n",
    "                else:\n",
    "                    other_embs.append(emb)\n",
    "                lr_weights.append(lr_w)\n",
    "            return inputs, ID_emb, item_embs, other_embs, tf.concat(lr_weights, -1), ID_table\n",
    "        def generate_meta_emb(item_embs):\n",
    "            embs = tf.stop_gradient(tf.stack(item_embs, 1))\n",
    "            item_h = tf.reduce_mean(embs, 1)\n",
    "            item_h2 = tf.layers.flatten(embs)\n",
    "            h_Dense = tf.layers.Dense(\n",
    "                emb_size, activation=tf.nn.tanh, \n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-7),\n",
    "                name='emb_h')\n",
    "            h2 = h_Dense(item_h2)\n",
    "            emb_pred_Dense = tf.layers.Dense(\n",
    "                emb_size, activation=tf.nn.sigmoid, use_bias=False,\n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                name='emb_predictor')\n",
    "            emb_pred = emb_pred_Dense(item_h + h2)\n",
    "            ME_vars = h_Dense.trainable_variables+emb_pred_Dense.trainable_variables\n",
    "            return emb_pred/5., ME_vars\n",
    "        \"\"\"\n",
    "        Base models:\n",
    "            FM, MLP, deepFM, IPNN, OPNN, PNNstar, wide&deep\n",
    "        \"\"\"\n",
    "        def latent_FM(embeddings, lr_weights=None, **kwargs):\n",
    "            sum_of_emb = tf.reduce_sum(embeddings, 1, keepdims=True)\n",
    "            diff_of_emb = sum_of_emb - embeddings\n",
    "            dots = tf.reduce_sum(embeddings*diff_of_emb, axis=-1)\n",
    "            if 'scaling_factor' in kwargs:\n",
    "                dots = dots * kwargs['scaling_factor']\n",
    "            biases = tf.reduce_sum(embeddings, -1)\n",
    "            fm_latent = tf.concat([dots, biases], 1)\n",
    "            return fm_latent\n",
    "        def latent_MLP_concat(embeddings, lr_weights=None, **kwargs):\n",
    "            h = tf.layers.flatten(embeddings)\n",
    "            if kwargs.get('n_hidden', 0)>0:\n",
    "                for i in range(kwargs['n_hidden']):\n",
    "                    h = tf.nn.relu(tf.layers.dense(\n",
    "                        h, kwargs.get('dense_size', 200), \n",
    "                        kernel_initializer=tf.initializers.he_uniform(),\n",
    "                        kernel_regularizer=l2_reg(reg_lambda),\n",
    "                        name='MLP-fc{}'.format(i)))\n",
    "            return h\n",
    "        def latent_MLP_avg(embeddings, lr_weights=None, **kwargs):\n",
    "            h = tf.reduce_mean(embeddings, -1)\n",
    "            if kwargs.get('n_hidden', 0)>0:\n",
    "                for i in range(kwargs['n_hidden']):\n",
    "                    h = tf.nn.relu(tf.layers.dense(\n",
    "                        h, kwargs.get('dense_size', 200), \n",
    "                        kernel_initializer=tf.initializers.he_uniform(),\n",
    "                        kernel_regularizer=l2_reg(reg_lambda),\n",
    "                        name='MLP-fc{}'.format(i)))\n",
    "            return h\n",
    "        def latent_deepFM(embeddings, lr_weights=None, **kwargs):\n",
    "            if 'n_hidden' not in kwargs:\n",
    "                kwargs['n_hidden'] = 3\n",
    "            fm_h = latent_FM(embeddings, **kwargs)\n",
    "            deep_h = latent_MLP_concat(embeddings, **kwargs)\n",
    "            h = tf.concat([fm_h, deep_h], 1)\n",
    "            return h\n",
    "        def latent_PNN(embeddings, lr_weights=None, **kwargs):\n",
    "            fm_h = latent_FM(embeddings, **kwargs)\n",
    "            h = tf.nn.relu(tf.layers.dense(fm_h, emb_size, name='fc', \n",
    "                                           kernel_initializer=tf.initializers.he_uniform(),\n",
    "                                           kernel_regularizer=l2_reg(reg_lambda),))\n",
    "            return h\n",
    "        def latent_OPNN(embeddings, lr_weights=None, **kwargs):\n",
    "            sum_of_emb = tf.reduce_sum(embeddings, 1)\n",
    "            a = tf.expand_dims(sum_of_emb, -1)\n",
    "            b = tf.expand_dims(sum_of_emb, 1)\n",
    "            outer = a*b\n",
    "            h = tf.layers.flatten(outer)\n",
    "            biases = tf.reduce_sum(embeddings, -1)\n",
    "            h = tf.concat([h, biases], 1)\n",
    "            h = tf.nn.relu(tf.layers.dense(h, emb_size, name='fc', \n",
    "                                           kernel_initializer=tf.initializers.he_uniform(),\n",
    "                                           kernel_regularizer=l2_reg(reg_lambda),))\n",
    "            return h\n",
    "        def latent_PNNstar(embeddings, lr_weights=None, **kwargs):\n",
    "            sum_of_emb = tf.reduce_sum(embeddings, 1)\n",
    "            a = tf.expand_dims(sum_of_emb, -1)\n",
    "            b = tf.expand_dims(sum_of_emb, 1)\n",
    "            outer = a*b\n",
    "            h = tf.layers.flatten(outer)\n",
    "            biases = tf.reduce_sum(embeddings, -1)\n",
    "            fm_h = latent_FM(embeddings, **kwargs)\n",
    "            h = tf.concat([h, fm_h, biases], 1)\n",
    "            h = tf.nn.relu(tf.layers.dense(h, emb_size, name='fc', \n",
    "                                           kernel_initializer=tf.initializers.he_uniform(),\n",
    "                                           kernel_regularizer=l2_reg(reg_lambda),))\n",
    "            return h\n",
    "        def latent_widendeep(embeddings, lr_weights=None, **kwargs):\n",
    "            if 'n_hidden' not in kwargs:\n",
    "                kwargs['n_hidden'] = 2\n",
    "            deep_h = latent_MLP_concat(embeddings, **kwargs)\n",
    "            lr_out = tf.reduce_sum(lr_weights, -1, keepdims=True)\n",
    "            return tf.concat([deep_h, lr_out], 1)\n",
    "        def get_yhat(ID_emb, item_embs, other_embs, lr_weights):\n",
    "            embeddings = tf.stack([ID_emb] + item_embs + other_embs, 1)\n",
    "            h = get_latent(embeddings, lr_weights)\n",
    "            return tf.sigmoid(tf.layers.dense(h, 1, name='out'))[:,0]\n",
    "        '''\n",
    "        *CHOOSE THE BASE MODEL HERE*\n",
    "        '''\n",
    "        models = {\n",
    "            'FM': latent_FM,\n",
    "            'MLP_concat': latent_MLP_concat,\n",
    "            'MLP_avg': latent_MLP_avg,\n",
    "            'deepFM': latent_deepFM,\n",
    "            'IPNN': latent_PNN,\n",
    "            'OPNN': latent_OPNN,\n",
    "            'PNNstar': latent_PNNstar,\n",
    "            'widendeep': latent_widendeep,\n",
    "        }\n",
    "        get_latent = models[model_name]\n",
    "        '''\n",
    "        *CHOOSE THE BASE MODEL HERE*\n",
    "        '''\n",
    "        with tf.variable_scope(\"model\"):\n",
    "            # build the base model\n",
    "            inputs, ID_emb, item_embs, other_embs, lr_weights, ID_table = get_embeddings()\n",
    "            label = tf.placeholder(tf.float32, [None])\n",
    "            # outputs and losses of the base model\n",
    "            yhat = get_yhat(ID_emb, item_embs, other_embs, lr_weights)\n",
    "            warm_loss = tf.losses.log_loss(label, yhat)\n",
    "            # Meta-Embedding: build the embedding generator\n",
    "            meta_ID_emb, ME_vars = generate_meta_emb(item_embs)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            # Meta-Embedding: step 1, cold-start, \n",
    "            #     use the generated meta-embedding to make predictions\n",
    "            #     and calculate the cold-start loss_a\n",
    "            cold_yhat_a = get_yhat(meta_ID_emb, item_embs, other_embs, lr_weights)\n",
    "            cold_loss_a = tf.losses.log_loss(label, cold_yhat_a)\n",
    "            # Meta-Embedding: step 2, apply gradient descent once\n",
    "            #     get the adapted embedding\n",
    "            cold_emb_grads = tf.gradients(cold_loss_a, meta_ID_emb)[0]\n",
    "            meta_ID_emb_new = meta_ID_emb - cold_lr * cold_emb_grads\n",
    "            # Meta-Embedding: step 3, \n",
    "            #     use the adapted embedding to make prediction on another mini-batch \n",
    "            #     and calculate the warm-up loss_b\n",
    "            inputs_b, _, item_embs_b, other_embs_b, lr_weights_b, _ = get_embeddings()\n",
    "            label_b = tf.placeholder(tf.float32, [None])\n",
    "            cold_yhat_b = get_yhat(meta_ID_emb_new, item_embs_b, other_embs_b, lr_weights_b)\n",
    "            cold_loss_b = tf.losses.log_loss(label_b, cold_yhat_b)            \n",
    "        \n",
    "        # build the optimizer and update op for the original model\n",
    "        warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "        warm_update_op = warm_optimizer.minimize(warm_loss)\n",
    "        warm_update_emb_op = warm_optimizer.minimize(warm_loss, var_list=[ID_table])\n",
    "        # build the optimizer and update op for meta-embedding\n",
    "        # Meta-Embedding: step 4, calculate the final meta-loss\n",
    "        ME_loss = cold_loss_a * alpha + cold_loss_b * (1-alpha)\n",
    "        ME_optimizer = tf.train.AdamOptimizer(ME_lr)\n",
    "        ME_update_op = ME_optimizer.minimize(cold_loss_b, var_list=ME_vars)\n",
    "        \n",
    "        ID_table_new = tf.placeholder(tf.float32, ID_table.shape)\n",
    "        ME_assign_op = tf.assign(ID_table, ID_table_new)\n",
    "        \n",
    "        def train_warm(sess, X, y, embedding_only=False):\n",
    "            # original training on batch\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict[label] = y\n",
    "            return sess.run([\n",
    "                warm_loss, warm_update_emb_op if embedding_only else warm_update_op \n",
    "            ], feed_dict=feed_dict)\n",
    "        def predict_warm(sess, X):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            return sess.run(yhat, feed_dict)\n",
    "        def predict_ME(sess, X):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            return sess.run(cold_yhat_a, feed_dict)\n",
    "        def get_meta_embedding(sess, X):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            return sess.run(meta_ID_emb, feed_dict)\n",
    "        def assign_meta_embedding(sess, ID, emb):\n",
    "            # take the embedding matrix\n",
    "            table = sess.run(ID_table)\n",
    "            # replace the ID^th row by the new embedding\n",
    "            table[ID, :] = emb\n",
    "            return sess.run(ME_assign_op, feed_dict={ID_table_new: table})\n",
    "        def train_ME(sess, X, y, X_b, y_b):\n",
    "            # train the embedding generator\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict[label] = y\n",
    "            feed_dict_b = {inputs_b[col]: X_b[col] for col in columns}\n",
    "            feed_dict_b[label_b] = y_b\n",
    "            return sess.run([\n",
    "                cold_loss_a, cold_loss_b, ME_update_op\n",
    "            ], feed_dict={**feed_dict, **feed_dict_b})\n",
    "        self.predict_warm = predict_warm\n",
    "        self.predict_ME = predict_ME\n",
    "        self.train_warm = train_warm\n",
    "        self.train_ME = train_ME\n",
    "        self.get_meta_embedding = get_meta_embedding\n",
    "        self.assign_meta_embedding = assign_meta_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:02:06.756257Z",
     "start_time": "2019-06-15T08:01:57.435347Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, maxlen_dict=maxlen_dict,\n",
    "                   model_name=MODEL, \n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:02:06.764501Z",
     "start_time": "2019-06-15T08:02:06.758744Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(sess, predict_func, test_x, batchsize=800):\n",
    "    n_samples_test = len(test_x[cols[0]])\n",
    "    n_batch_test = n_samples_test//batchsize\n",
    "    if n_batch_test*batchsize < n_samples_test:\n",
    "        n_batch_test += 1\n",
    "    test_pred = np.zeros(n_samples_test)\n",
    "    for i_batch in range(n_batch_test):\n",
    "        b, e = i_batch*batchsize, min((i_batch+1)*batchsize, n_samples_test)\n",
    "        batch_x = {k: test_x[k][b:e] for k in test_x}\n",
    "        _pred = predict_func(sess, batch_x)\n",
    "        test_pred[b:e] = _pred.reshape(-1)\n",
    "    return test_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:02:38.688597Z",
     "start_time": "2019-06-15T08:02:06.766775Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [00:29<00:00, 128.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train]\n",
      "\ttest-test loss: 1.535251\n",
      "[pre-train]\n",
      "\ttest-test auc: 0.642586\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pre-train the base model\n",
    "\"\"\"\n",
    "batchsize = BATCHSIZE\n",
    "n_epoch = 1\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_samples = len(train_y)\n",
    "n_batch = n_samples//batchsize\n",
    "for i_epoch in range(n_epoch):\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        b, e = i_batch*batchsize, (i_batch+1)*batchsize\n",
    "        batch_x = {k: train_x[k][b:e] for k in train_x}\n",
    "        batch_y = train_y[b:e]\n",
    "        loss, _ = model.train_warm(sess, batch_x, batch_y)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_base_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:02:57.867511Z",
     "start_time": "2019-06-15T08:02:57.863351Z"
    }
   },
   "outputs": [],
   "source": [
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize*batch_n_ID\n",
    "n_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:15:27.923946Z",
     "start_time": "2019-06-15T08:15:23.604869Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 113.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.537740\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.660663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 104.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.518446\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.658049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 107.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.537248\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.662824\n",
      "Model saved in path: saver/modelFM\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the Meta-Embedding generator\n",
    "'''\n",
    "best_auc = 0\n",
    "best_loss = 10\n",
    "for i_epoch in range(n_epoch):\n",
    "    # Read the few-shot training data of big ads\n",
    "    if i_epoch==0:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "    elif i_epoch==1:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "    elif i_epoch==2:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "    elif i_epoch==3:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "    train_x_a = _train_a[cols].to_dict('l')\n",
    "    train_y_a = _train_a['y'].values\n",
    "    for col in maxlen_dict:\n",
    "        train_x_a[col] = pad_sequences(train_x_a[col], maxlen=maxlen_dict[col])\n",
    "    train_x_b = _train_b[cols].to_dict('l')\n",
    "    train_y_b = _train_b['y'].values\n",
    "    for col in maxlen_dict:\n",
    "        train_x_b[col] = pad_sequences(train_x_b[col], maxlen=maxlen_dict[col])\n",
    "\n",
    "    n_samples = len(train_y_a)\n",
    "    n_batch = n_samples//batchsize\n",
    "    # Start training\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        b, e = i_batch*batchsize, (i_batch+1)*batchsize\n",
    "        batch_x_a = {k: train_x_a[k][b:e] for k in train_x_a}\n",
    "        batch_y_a = train_y_a[b:e]\n",
    "        batch_x_b = {k: train_x_b[k][b:e] for k in train_x_b}\n",
    "        batch_y_b = train_y_b[b:e]\n",
    "        loss_a, loss_b, _ = model.train_ME(sess, batch_x_a, batch_y_a, batch_x_b, batch_y_b, )\n",
    "    # on epoch end\n",
    "    test_pred_test = predict_on_batch(sess, model.predict_ME, test_x_test,)\n",
    "    logloss_ME_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "    auc_ME_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "save_path = saver.save(sess, saver_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T08:15:38.328321Z",
     "start_time": "2019-06-15T08:15:29.201872Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saver/modelFM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 228.40it/s]\n",
      " 50%|█████     | 23/46 [00:00<00:00, 229.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.388621\n",
      "[baseline]\n",
      "\ttest-test auc: 0.687931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 227.75it/s]\n",
      " 48%|████▊     | 22/46 [00:00<00:00, 213.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.373638\n",
      "[baseline]\n",
      "\ttest-test auc: 0.690739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 213.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.361268\n",
      "[baseline]\n",
      "\ttest-test auc: 0.693102\n",
      "============================================================\n",
      "INFO:tensorflow:Restoring parameters from saver/modelFM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:05<00:00,  8.99it/s]\n",
      " 46%|████▌     | 21/46 [00:00<00:00, 207.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.496560\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.668737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 214.93it/s]\n",
      " 50%|█████     | 23/46 [00:00<00:00, 224.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.464926\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.674226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 234.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.441446\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.678382\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing\n",
    "'''\n",
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize * batch_n_ID\n",
    "i = 1\n",
    "test_n_ID = len(np.unique(test_x_a[ID_col]))\n",
    "saver.restore(sess, save_path)\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_a[k][b:e] for k in cols}\n",
    "    batch_y = test_y_a[b:e]\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_base_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_b[k][b:e] for k in cols}\n",
    "    batch_y = test_y_b[b:e]\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_base_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_c[k][b:e] for k in cols}\n",
    "    batch_y = test_y_c[b:e]\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_base_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "print(\"=\"*60)\n",
    "\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_a[k][b:e] for k in cols}\n",
    "    batch_y = test_y_a[b:e]\n",
    "    aid = np.unique(batch_x[ID_col])\n",
    "    for k in range(batch_n_ID):\n",
    "        if k*minibatchsize>=len(batch_y):\n",
    "            break\n",
    "        ID = batch_x[ID_col][k*minibatchsize]\n",
    "        embeddings = model.get_meta_embedding(\n",
    "            sess, {c: batch_x[c][k*minibatchsize:(k+1)*minibatchsize] for c in cols})\n",
    "        emb = embeddings.mean(0)\n",
    "        model.assign_meta_embedding(sess, ID, emb)\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_ME_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_b[k][b:e] for k in cols}\n",
    "    batch_y = test_y_b[b:e]\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_ME_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    b, e = i*batchsize, (i+1)*batchsize\n",
    "    batch_x = {k: test_x_c[k][b:e] for k in cols}\n",
    "    batch_y = test_y_c[b:e]\n",
    "    model.train_warm(sess, batch_x, batch_y, embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, test_x_test)\n",
    "logloss_ME_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T12:23:42.377052Z",
     "start_time": "2019-06-14T12:23:42.372242Z"
    }
   },
   "outputs": [],
   "source": [
    "res = [logloss_base_cold, logloss_ME_cold, \n",
    "       logloss_base_batcha, logloss_ME_batcha, \n",
    "       logloss_base_batchb, logloss_ME_batchb, \n",
    "       logloss_base_batchc, logloss_ME_batchc, \n",
    "       auc_base_cold, auc_ME_cold, \n",
    "       auc_base_batcha, auc_ME_batcha, \n",
    "       auc_base_batchb, auc_ME_batchb, \n",
    "       auc_base_batchc, auc_ME_batchc]\n",
    "with open(LOG, \"a\") as logfile:\n",
    "    logfile.writelines(\",\".join([str(x) for x in res])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
