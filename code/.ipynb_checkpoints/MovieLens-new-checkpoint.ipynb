{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:07.462231Z",
     "start_time": "2019-06-14T10:57:04.988528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panfy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.layers import Dense\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:07.469049Z",
     "start_time": "2019-06-14T10:57:07.464336Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Config\n",
    "'''\n",
    "# batch size per iteration\n",
    "BATCHSIZE = 200\n",
    "# mini-batch size for few-shot learning\n",
    "MINIBATCHSIZE = 20 \n",
    "# learning rate\n",
    "LR = 1e-3 \n",
    "# coefficient to balance `cold-start' and `warm-up'\n",
    "ALPHA = 0.1\n",
    "# length of embedding vectors\n",
    "EMB_SIZE = 256\n",
    "# log file\n",
    "LOG = \"logs/FM256.csv\"\n",
    "# path to save the model\n",
    "saver_path =\"saver/model\"+LOG.split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:07.476358Z",
     "start_time": "2019-06-14T10:57:07.471683Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        t = pickle.load(f)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:15.080152Z",
     "start_time": "2019-06-14T10:57:07.478093Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data of big ads\n",
    "train = read_pkl(\"../data/big_train_main.pkl\")\n",
    "# some pre-processing\n",
    "num_words_dict = {\n",
    "    'MovieID': 4000,\n",
    "    'UserID': 6050,\n",
    "    'Age': 7,\n",
    "    'Gender': 2,\n",
    "    'Occupation': 21,\n",
    "    'Year': 83,\n",
    "}\n",
    "ID_col = 'MovieID'\n",
    "item_col = ['Year']\n",
    "context_col = ['Age', 'Gender', 'Occupation', 'UserID']\n",
    "train_y = train['y']\n",
    "train_x = train[[ID_col]+item_col+context_col]\n",
    "train_t = pad_sequences(train.Title, maxlen=8)\n",
    "train_g = pad_sequences(train.Genres, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:16.919410Z",
     "start_time": "2019-06-14T10:57:15.083775Z"
    }
   },
   "outputs": [],
   "source": [
    "# few-shot data\n",
    "test_a = read_pkl(\"../data/test_oneshot_a.pkl\")\n",
    "test_b = read_pkl(\"../data/test_oneshot_b.pkl\")\n",
    "test_c = read_pkl(\"../data/test_oneshot_c.pkl\")\n",
    "test_test = read_pkl(\"../data/test_test.pkl\")\n",
    "\n",
    "test_x_a = test_a[[ID_col]+item_col+context_col]\n",
    "test_y_a = test_a['y'].values\n",
    "test_t_a = pad_sequences(test_a.Title, maxlen=8)\n",
    "test_g_a = pad_sequences(test_a.Genres, maxlen=4)\n",
    "\n",
    "test_x_b = test_b[[ID_col]+item_col+context_col]\n",
    "test_y_b = test_b['y'].values\n",
    "test_t_b = pad_sequences(test_b.Title, maxlen=8)\n",
    "test_g_b = pad_sequences(test_b.Genres, maxlen=4)\n",
    "\n",
    "test_x_c = test_c[[ID_col]+item_col+context_col]\n",
    "test_y_c = test_c['y'].values\n",
    "test_t_c = pad_sequences(test_c.Title, maxlen=8)\n",
    "test_g_c = pad_sequences(test_c.Genres, maxlen=4)\n",
    "\n",
    "test_x_test = test_test[[ID_col]+item_col+context_col]\n",
    "test_y_test = test_test['y'].values\n",
    "test_t_test = pad_sequences(test_test.Title, maxlen=8)\n",
    "test_g_test = pad_sequences(test_test.Genres, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:16.978210Z",
     "start_time": "2019-06-14T10:57:16.922349Z"
    },
    "code_folding": [
     12,
     41,
     61,
     76,
     84,
     109,
     128,
     146,
     169
    ]
   },
   "outputs": [],
   "source": [
    "class Meta_Model(object):\n",
    "    def __init__(self, ID_col, item_col, context_col, nb_words,\n",
    "                 emb_size=128, alpha=0.2,\n",
    "                 warm_lr=1e-3, cold_lr=1e-4, ME_lr=1e-3):\n",
    "        \"\"\"\n",
    "        ID_col: string, the column name of the item ID\n",
    "        item_col: list, the columns of item features\n",
    "        context_col: list, the columns of other features\n",
    "        nb_words: dict, nb of words in each of these columns\n",
    "        get_yhat: the base model, default DeepFM\n",
    "        \"\"\"\n",
    "        columns = [ID_col] + item_col + context_col\n",
    "        def get_embeddings():\n",
    "            inputs, tables = {}, []\n",
    "            item_embs, other_embs = [], []\n",
    "            for col in columns:\n",
    "                inputs[col] = tf.placeholder(tf.int32, [None])\n",
    "                table = tf.get_variable(\n",
    "                    \"table_{}\".format(col), [nb_words[col], emb_size],\n",
    "                    initializer=tf.random_normal_initializer(stddev=0.01),\n",
    "                    regularizer=tf.contrib.layers.l2_regularizer(1e-7))\n",
    "                emb = tf.nn.embedding_lookup(table, inputs[col])\n",
    "                if col==ID_col:\n",
    "                    ID_emb = emb\n",
    "                    ID_table = table\n",
    "                elif col in item_col:\n",
    "                    item_embs.append(emb)\n",
    "                else:\n",
    "                    other_embs.append(emb)\n",
    "\n",
    "            inputs[\"title\"] = tf.placeholder(tf.int32, [None, 8])\n",
    "            inputs[\"genres\"] = tf.placeholder(tf.int32, [None, 4])\n",
    "\n",
    "            title_emb = tf.contrib.layers.embed_sequence(\n",
    "                inputs[\"title\"], 20001, emb_size, scope=\"word_emb\")\n",
    "            genre_emb = tf.contrib.layers.embed_sequence(\n",
    "                inputs[\"genres\"], 21, emb_size, scope=\"genre_table\")\n",
    "            item_embs.append(tf.reduce_mean(title_emb, axis=1))\n",
    "            item_embs.append(tf.reduce_mean(genre_emb, axis=1))\n",
    "            \n",
    "            return inputs, ID_emb, item_embs, other_embs, ID_table\n",
    "        def generate_meta_emb(item_embs):\n",
    "            item_h = tf.stop_gradient(tf.add_n(item_embs))\n",
    "            item_h2 = tf.stop_gradient(tf.concat(item_embs, 1))\n",
    "            h_Dense = tf.layers.Dense(\n",
    "                emb_size, activation=tf.nn.tanh, \n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-7),\n",
    "                name='emb_h')\n",
    "            h2 = h_Dense(item_h2)\n",
    "            emb_pred_Dense = tf.layers.Dense(\n",
    "                emb_size, activation=tf.nn.sigmoid, use_bias=False,\n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3),\n",
    "                name='emb_predictor')\n",
    "            emb_pred = emb_pred_Dense(item_h + h2)\n",
    "            ME_vars = h_Dense.trainable_variables+emb_pred_Dense.trainable_variables\n",
    "            return emb_pred/5., ME_vars\n",
    "        \"\"\"\n",
    "        Base models:\n",
    "            FM, MLP, deepFM, IPNN, OPNN, PNNstar, wide&deep\n",
    "        \"\"\"\n",
    "\n",
    "        def get_yhat_FM(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True) \n",
    "                          for i in range(len(columns))]\n",
    "            dots = tf.concat(dot_of_emb, 1)\n",
    "            bias = tf.get_variable('fm_bias', shape=(dots.shape[1],))\n",
    "            h = tf.concat([dots, dots+bias], 1)\n",
    "            w = tf.get_variable('FM_out/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('FM_out/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(.03))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_MLP(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            h = tf.concat(embeddings, 1)\n",
    "            w = tf.get_variable('MLP/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('MLP/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(.03))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_deepFM(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True) \n",
    "                          for i in range(len(columns))]\n",
    "            dots = tf.concat(dot_of_emb, 1)\n",
    "            bias = tf.get_variable('fm_bias', shape=(dots.shape[1],))\n",
    "            h = tf.concat([dots, dots+bias], 1)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            w = tf.get_variable('MLP/kernel', shape=(h2.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP/bias', shape=(emb_size,), initializer=tf.initializers.zeros)\n",
    "            h2 = tf.nn.relu(tf.matmul(h2,w)+b)\n",
    "            for i in range(2):\n",
    "                w = tf.get_variable('MLP_{}/kernel'.format(i+1), shape=(emb_size, emb_size))\n",
    "                b = tf.get_variable('MLP_{}/bias'.format(i+1), shape=(emb_size,), \n",
    "                                    initializer=tf.initializers.zeros)\n",
    "                h2 = tf.nn.relu(tf.matmul(h2,w)+b)\n",
    "            h = tf.concat([h,h2], 1)\n",
    "            w = tf.get_variable('deepFM_out/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('deepFM_out/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_PNN(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True)\n",
    "                          for i in range(len(columns))]\n",
    "            dots = tf.concat(dot_of_emb, 1)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            h = tf.concat([dots,h2], 1)\n",
    "            w = tf.get_variable('MLP_1/kernel', shape=(h.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP_1/bias', shape=(emb_size,), \n",
    "                                initializer=tf.initializers.zeros)\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b)\n",
    "            w = tf.get_variable('MLP_2/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('MLP_2/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_OPNN(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            a = tf.expand_dims(sum_of_emb, -1)\n",
    "            b = tf.expand_dims(sum_of_emb, 1)\n",
    "            outer = a*b\n",
    "            h = tf.layers.flatten(outer)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            h = tf.concat([h,h2], 1)\n",
    "            w = tf.get_variable('MLP_1/kernel', shape=(h.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP_1/bias', shape=(emb_size,), \n",
    "                                initializer=tf.initializers.zeros)\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b)\n",
    "            w = tf.get_variable('MLP_2/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('MLP_2/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_PNNstar(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            a = tf.expand_dims(sum_of_emb, -1)\n",
    "            b = tf.expand_dims(sum_of_emb, 1)\n",
    "            outer = a*b\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True)\n",
    "                          for i in range(len(columns))]\n",
    "            dots = tf.concat(dot_of_emb, 1)\n",
    "            h = tf.concat([dots, tf.layers.flatten(outer)], 1)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            h = tf.concat([h,h2], 1)\n",
    "            w = tf.get_variable('MLP_1/kernel', shape=(h.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP_1/bias', shape=(emb_size,), \n",
    "                                initializer=tf.initializers.zeros)\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b)\n",
    "            w = tf.get_variable('MLP_2/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('MLP_2/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        def get_yhat_widendeep(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            w = tf.get_variable('MLP/kernel', shape=(h2.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP/bias', shape=(emb_size,), initializer=tf.initializers.zeros)\n",
    "            h2 = tf.nn.relu(tf.matmul(h2,w)+b)\n",
    "            for i in range(2):\n",
    "                w = tf.get_variable('MLP_{}/kernel'.format(i+1), shape=(emb_size,emb_size))\n",
    "                b = tf.get_variable('MLP_{}/bias'.format(i+1), shape=(emb_size,), \n",
    "                                    initializer=tf.initializers.zeros)\n",
    "                h2 = tf.nn.relu(tf.matmul(h2,w)+b)\n",
    "            h = h2\n",
    "            w = tf.get_variable('wd_out/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('wd_out/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        \n",
    "        '''\n",
    "        *CHOOSE THE BASE MODEL HERE*\n",
    "        '''\n",
    "        get_yhat = get_yhat_deepFM\n",
    "        \n",
    "        with tf.variable_scope(\"model\"):\n",
    "            # build the base model\n",
    "            inputs, ID_emb, item_embs, other_embs, ID_table = get_embeddings()\n",
    "            label = tf.placeholder(tf.float32, [None, 1])\n",
    "            # outputs and losses of the base model\n",
    "            yhat = get_yhat(ID_emb, item_embs, other_embs)\n",
    "            warm_loss = tf.losses.log_loss(label, yhat)\n",
    "            # Meta-Embedding: build the embedding generator\n",
    "            meta_ID_emb, ME_vars = generate_meta_emb(item_embs)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            # Meta-Embedding: step 1, cold-start, \n",
    "            #     use the generated meta-embedding to make predictions\n",
    "            #     and calculate the cold-start loss_a\n",
    "            cold_yhat_a = get_yhat(meta_ID_emb, item_embs, other_embs)\n",
    "            cold_loss_a = tf.losses.log_loss(label, cold_yhat_a)\n",
    "            # Meta-Embedding: step 2, apply gradient descent once\n",
    "            #     get the adapted embedding\n",
    "            cold_emb_grads = tf.gradients(cold_loss_a, meta_ID_emb)[0]\n",
    "            meta_ID_emb_new = meta_ID_emb - cold_lr * cold_emb_grads\n",
    "            # Meta-Embedding: step 3, \n",
    "            #     use the adapted embedding to make prediction on another mini-batch \n",
    "            #     and calculate the warm-up loss_b\n",
    "            inputs_b, _, item_embs_b, other_embs_b, _ = get_embeddings()\n",
    "            label_b = tf.placeholder(tf.float32, [None, 1])\n",
    "            cold_yhat_b = get_yhat(meta_ID_emb_new, item_embs_b, other_embs_b)\n",
    "            cold_loss_b = tf.losses.log_loss(label_b, cold_yhat_b)            \n",
    "        \n",
    "        # build the optimizer and update op for the original model\n",
    "        warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "        warm_update_op = warm_optimizer.minimize(warm_loss)\n",
    "        warm_update_emb_op = warm_optimizer.minimize(warm_loss, var_list=[ID_table])\n",
    "        # build the optimizer and update op for meta-embedding\n",
    "        # Meta-Embedding: step 4, calculate the final meta-loss\n",
    "        ME_loss = cold_loss_a * alpha + cold_loss_b * (1-alpha)\n",
    "        ME_optimizer = tf.train.AdamOptimizer(ME_lr)\n",
    "        ME_update_op = ME_optimizer.minimize(cold_loss_b, var_list=ME_vars)\n",
    "        \n",
    "        ID_table_new = tf.placeholder(tf.float32, ID_table.shape)\n",
    "        ME_assign_op = tf.assign(ID_table, ID_table_new)\n",
    "        \n",
    "        def predict_warm(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(yhat, feed_dict)\n",
    "        def predict_ME(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(cold_yhat_a, feed_dict)\n",
    "        def get_meta_embedding(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(meta_ID_emb, feed_dict)\n",
    "        def assign_meta_embedding(sess, ID, emb):\n",
    "            # take the embedding matrix\n",
    "            table = sess.run(ID_table)\n",
    "            # replace the ID^th row by the new embedding\n",
    "            table[ID, :] = emb\n",
    "            return sess.run(ME_assign_op, feed_dict={ID_table_new: table})\n",
    "        def train_warm(sess, X, Title, Genres, y, embedding_only=False):\n",
    "            # original training on batch\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                warm_loss, warm_update_emb_op if embedding_only else warm_update_op \n",
    "            ], feed_dict=feed_dict)\n",
    "        def train_ME(sess, X, Title, Genres, y, \n",
    "                     X_b, Title_b, Genres_b, y_b):\n",
    "            # train the embedding generator\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            feed_dict_b = {inputs_b[col]: X_b[col] for col in columns}\n",
    "            feed_dict_b = {inputs_b[\"title\"]: Title_b,\n",
    "                           inputs_b[\"genres\"]: Genres_b,\n",
    "                           **feed_dict_b}\n",
    "            feed_dict_b[label_b] = y_b.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                cold_loss_a, cold_loss_b, ME_update_op\n",
    "            ], feed_dict={**feed_dict, **feed_dict_b})\n",
    "        self.predict_warm = predict_warm\n",
    "        self.predict_ME = predict_ME\n",
    "        self.train_warm = train_warm\n",
    "        self.train_ME = train_ME\n",
    "        self.get_meta_embedding = get_meta_embedding\n",
    "        self.assign_meta_embedding = assign_meta_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:27.661256Z",
     "start_time": "2019-06-14T10:57:16.981413Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:57:27.670515Z",
     "start_time": "2019-06-14T10:57:27.663924Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(sess, predict_func, test_x, test_t, test_g, batchsize=800):\n",
    "    n_samples_test = test_x.shape[0]\n",
    "    n_batch_test = n_samples_test//batchsize\n",
    "    test_pred = np.zeros(n_samples_test)\n",
    "    for i_batch in range(n_batch_test):\n",
    "        batch_x = test_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = test_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g = test_g[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_g)\n",
    "        test_pred[i_batch*batchsize:(i_batch+1)*batchsize] = _pred.reshape(-1)\n",
    "    if n_batch_test*batchsize<n_samples_test:\n",
    "        batch_x = test_x.iloc[n_batch_test*batchsize:]\n",
    "        batch_t = test_t[n_batch_test*batchsize:]\n",
    "        batch_g = test_g[n_batch_test*batchsize:]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_g)\n",
    "        test_pred[n_batch_test*batchsize:] = _pred.reshape(-1)\n",
    "    return test_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:58:09.451609Z",
     "start_time": "2019-06-14T10:57:27.672668Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [00:39<00:00, 97.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train]\n",
      "\ttest-test loss: 1.181204\n",
      "[pre-train]\n",
      "\ttest-test auc: 0.650819\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pre-train the base model\n",
    "\"\"\"\n",
    "batchsize = BATCHSIZE\n",
    "n_epoch = 1\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_samples = train_x.shape[0]\n",
    "n_batch = n_samples//batchsize\n",
    "for i_epoch in range(n_epoch):\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        batch_x = train_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = train_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g = train_g[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y = train_y.iloc[i_batch*batchsize:(i_batch+1)*batchsize].values\n",
    "        loss, _ = model.train_warm(sess, batch_x, batch_t, batch_g, batch_y)\n",
    "    # on epoch end\n",
    "    pass\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:58:09.457079Z",
     "start_time": "2019-06-14T10:58:09.454063Z"
    }
   },
   "outputs": [],
   "source": [
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize*batch_n_ID\n",
    "n_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:58:16.813212Z",
     "start_time": "2019-06-14T10:58:09.459374Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:02<00:00, 19.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.101137\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.663830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 66.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 0.996458\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.667284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 0.982395\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.673454\n",
      "Model saved in path: saver/modelFM256\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the Meta-Embedding generator\n",
    "'''\n",
    "best_auc = 0\n",
    "best_loss = 10\n",
    "for i_epoch in range(n_epoch):\n",
    "    # Read the few-shot training data of big ads\n",
    "    if i_epoch==0:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "    elif i_epoch==1:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "    elif i_epoch==2:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "    elif i_epoch==3:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "    train_x_a = _train_a[[ID_col]+item_col+context_col]\n",
    "    train_y_a = _train_a['y'].values\n",
    "    train_t_a = pad_sequences(_train_a.Title, maxlen=8)\n",
    "    train_g_a = pad_sequences(_train_a.Genres, maxlen=4)\n",
    "\n",
    "    train_x_b = _train_b[[ID_col]+item_col+context_col]\n",
    "    train_y_b = _train_b['y'].values\n",
    "    train_t_b = pad_sequences(_train_b.Title, maxlen=8)\n",
    "    train_g_b = pad_sequences(_train_b.Genres, maxlen=4)\n",
    "    \n",
    "    n_samples = train_x_a.shape[0]\n",
    "    n_batch = n_samples//batchsize\n",
    "    # Start training\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        batch_x_a = train_x_a.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t_a = train_t_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g_a = train_g_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y_a = train_y_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_x_b = train_x_b.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t_b = train_t_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g_b = train_g_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y_b = train_y_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        loss_a, loss_b, _ = model.train_ME(sess, \n",
    "                                           batch_x_a, batch_t_a, batch_g_a, batch_y_a, \n",
    "                                           batch_x_b, batch_t_b, batch_g_b, batch_y_b, )\n",
    "    # on epoch end\n",
    "    test_pred_test = predict_on_batch(sess, model.predict_ME, \n",
    "                                      test_x_test, test_t_test, test_g_test)\n",
    "    logloss_ME_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "    auc_ME_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "save_path = saver.save(sess, saver_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:58:28.652932Z",
     "start_time": "2019-06-14T10:58:16.815132Z"
    },
    "code_folding": [
     76,
     93
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saver/modelFM256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 72.56it/s]\n",
      " 37%|███▋      | 17/46 [00:00<00:00, 163.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.106878\n",
      "[baseline]\n",
      "\ttest-test auc: 0.676565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 170.07it/s]\n",
      " 37%|███▋      | 17/46 [00:00<00:00, 163.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.058685\n",
      "[baseline]\n",
      "\ttest-test auc: 0.689485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 172.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss: 1.025457\n",
      "[baseline]\n",
      "\ttest-test auc: 0.696818\n",
      "============================================================\n",
      "INFO:tensorflow:Restoring parameters from saver/modelFM256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:06<00:00,  7.18it/s]\n",
      " 39%|███▉      | 18/46 [00:00<00:00, 175.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 0.924912\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.693340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 172.72it/s]\n",
      " 39%|███▉      | 18/46 [00:00<00:00, 178.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 0.891644\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.702861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 175.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 0.869810\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.708552\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing\n",
    "'''\n",
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize * batch_n_ID\n",
    "i = 1\n",
    "test_n_ID = len(test_x_c[ID_col].drop_duplicates())\n",
    "saver.restore(sess, save_path)\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_a[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_b[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_c[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "print(\"=\"*60)\n",
    "\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_a[i*batchsize:(i+1)*batchsize]\n",
    "    aid = np.unique(batch_x[ID_col].values)\n",
    "    for k in range(batch_n_ID):\n",
    "        if k*minibatchsize>=len(batch_x):\n",
    "            break\n",
    "        ID = batch_x[ID_col].values[k*minibatchsize]\n",
    "        embeddings = model.get_meta_embedding(\n",
    "            sess, batch_x[k*minibatchsize:(k+1)*minibatchsize],\n",
    "            batch_t[k*minibatchsize:(k+1)*minibatchsize],\n",
    "            batch_g[k*minibatchsize:(k+1)*minibatchsize],\n",
    "        )\n",
    "        emb = embeddings.mean(0)\n",
    "        model.assign_meta_embedding(sess, ID, emb)\n",
    "    model.train_warm(sess, \n",
    "                     batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_b[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_c[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_ME_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T10:58:28.663491Z",
     "start_time": "2019-06-14T10:58:28.655893Z"
    }
   },
   "outputs": [],
   "source": [
    "res = [logloss_base_cold, logloss_ME_cold, \n",
    "       logloss_base_batcha, logloss_ME_batcha, \n",
    "       logloss_base_batchb, logloss_ME_batchb, \n",
    "       logloss_base_batchc, logloss_ME_batchc, \n",
    "       auc_base_cold, auc_ME_cold, \n",
    "       auc_base_batcha, auc_ME_batcha, \n",
    "       auc_base_batchb, auc_ME_batchb, \n",
    "       auc_base_batchc, auc_ME_batchc]\n",
    "with open(LOG, \"a\") as logfile:\n",
    "    logfile.writelines(\",\".join([str(x) for x in res])+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
