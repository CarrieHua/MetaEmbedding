{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T05:45:44.251233Z",
     "start_time": "2018-12-29T05:45:43.001763Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from multiprocessing import Pool\n",
    "import pickle, gc, warnings, os, time\n",
    "get_current_time = lambda: time.strftime('%Y-%m-%d %X', time.localtime())\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "from argparse import ArgumentParser\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True, rc={'figure.figsize':(8,6)}, \n",
    "        font_scale=1.25, style='whitegrid', font='DejaVu Serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T05:45:51.714730Z",
     "start_time": "2018-12-29T05:45:47.254572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the MovieLens rating data\n",
    "train = pd.read_csv(\"../input/ratings.dat\", sep=\"::\", header=None, \n",
    "                    names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], \n",
    "                    usecols=[\"UserID\", \"MovieID\", \"Rating\"])\n",
    "train.to_feather(\"../input/ratings.feather\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T05:45:54.699154Z",
     "start_time": "2018-12-29T05:45:54.603981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the user data\n",
    "user_feature = pd.read_csv(\"../input/users.dat\", sep=\"::\", \n",
    "                           names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"],\n",
    "                           usecols=[\"UserID\", \"Gender\", \"Age\", \"Occupation\"])\n",
    "for col in [\"Gender\", \"Age\"]:\n",
    "    user_feature[col] = user_feature[col].astype(\"category\").cat.codes\n",
    "user_feature.to_feather(\"../input/users.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T05:47:57.810214Z",
     "start_time": "2018-12-29T05:47:56.667243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the movie data\n",
    "movie_feature = pd.read_csv(\"../input/movies.dat\", sep=\"::\",\n",
    "                            names=[\"MovieID\", \"Title\", \"Genres\"])\n",
    "# Pre-processing\n",
    "movie_feature[\"Year\"] = movie_feature[\"Title\"].apply(lambda t: int(t[-5:-1]))\n",
    "movie_feature[\"Year\"] = movie_feature[\"Year\"]-movie_feature[\"Year\"].min()+1\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20, split=\"|\")\n",
    "tokenizer.fit_on_texts(movie_feature[\"Genres\"])\n",
    "movie_feature[\"Genres\"] = tokenizer.texts_to_sequences(movie_feature[\"Genres\"])\n",
    "\n",
    "movie_feature[\"Title\"] = movie_feature[\"Title\"].apply(lambda t: t[:-7])\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(movie_feature[\"Title\"])\n",
    "movie_feature[\"Title\"] = tokenizer.texts_to_sequences(movie_feature[\"Title\"])\n",
    "\n",
    "movie_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T05:48:33.394015Z",
     "start_time": "2018-12-29T05:48:33.367145Z"
    }
   },
   "outputs": [],
   "source": [
    "# binarize the target column\n",
    "train['y'] = (train.Rating>=4).astype('int8')\n",
    "train = train[['UserID', 'MovieID', 'y']]\n",
    "# Merge the training data\n",
    "train = train.merge(user_feature, how='left')\n",
    "train = train.merge(movie_feature, how='left')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:07:22.269616Z",
     "start_time": "2018-12-29T06:07:21.657373Z"
    }
   },
   "outputs": [],
   "source": [
    "cnts = train.MovieID.value_counts()\n",
    "aid_counts = pd.DataFrame(cnts).reset_index()\n",
    "aid_counts.columns = [\"MovieID\", 'counts']\n",
    "aid_counts.reset_index(inplace=True)\n",
    "aids = train[[\"MovieID\"]].copy()\n",
    "aids = aids.merge(aid_counts, how='left')\n",
    "aids['prop'] = aids['index']/aids['index'].max()\n",
    "print(\"The total number of ad IDs: {}\".format(len(aid_counts)))\n",
    "\n",
    "sns.distplot(aids['prop'], kde=False, bins=20);\n",
    "plt.xlabel('proportion of movies');\n",
    "plt.ylabel('number of samples');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:07:52.899161Z",
     "start_time": "2018-12-29T06:07:52.885607Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the ad IDs\n",
    "# thresholds: 80 and 300\n",
    "# ads with at least 300 samples are \"big\" ones for pre-training the base model\n",
    "# ads with at least 80 samples are \"small\" ones for cold-start and warm-up\n",
    "# ads with less than 80 samples are no use for this experiment \n",
    "#     because we cannot build a testing dataset for them\n",
    "small_aids = cnts[(cnts<300) & (cnts>=80)].index.tolist()\n",
    "big_aids = cnts[cnts>=300].index.tolist()\n",
    "print(\"The number of small ads:\", len(small_aids))\n",
    "print(\"The number of big ads:\", len(big_aids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:12:42.371736Z",
     "start_time": "2018-12-29T06:12:41.981398Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "small_train = train[train.MovieID.isin(small_aids)]\n",
    "big_train = train[train.MovieID.isin(big_aids)]\n",
    "print(\"Shape of the small dataset:\\t{}\".format(small_train.shape))\n",
    "print(\"Shape of the big dataset:\\t{}\".format(big_train.shape))\n",
    "big_train.reset_index(drop=True, inplace=True)\n",
    "small_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:12:49.767311Z",
     "start_time": "2018-12-29T06:12:49.576567Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../ME_data/big_train_main.pkl', 'wb') as f:\n",
    "    pickle.dump(big_train, f)\n",
    "with open('../ME_data/small_train_main.pkl', 'wb') as f:\n",
    "    pickle.dump(small_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:14:39.072293Z",
     "start_time": "2018-12-29T06:14:36.700859Z"
    }
   },
   "outputs": [],
   "source": [
    "# pre-processing: get the few-shot batchs\n",
    "train_IDs = big_aids\n",
    "train_IDs_ind = {ID: big_train[big_train.MovieID==ID].index for ID in train_IDs}\n",
    "test_IDs = small_aids\n",
    "test_IDs_ind = {ID: small_train[small_train.MovieID==ID].index for ID in test_IDs}\n",
    "\n",
    "minibatchsize = 20\n",
    "train_oneshot_inds_a = {ID: train_IDs_ind[ID][:minibatchsize] for ID in train_IDs}\n",
    "train_oneshot_inds_a = np.concatenate(list(train_oneshot_inds_a.values()))\n",
    "train_oneshot_inds_b = {ID: train_IDs_ind[ID][minibatchsize:minibatchsize*2] for ID in train_IDs}\n",
    "train_oneshot_inds_b = np.concatenate(list(train_oneshot_inds_b.values()))\n",
    "train_oneshot_inds_c = {ID: train_IDs_ind[ID][2*minibatchsize:3*minibatchsize] for ID in train_IDs}\n",
    "train_oneshot_inds_c = np.concatenate(list(train_oneshot_inds_c.values()))\n",
    "train_oneshot_inds_d = {ID: train_IDs_ind[ID][3*minibatchsize:4*minibatchsize] for ID in train_IDs}\n",
    "train_oneshot_inds_d = np.concatenate(list(train_oneshot_inds_d.values()))\n",
    "test_oneshot_inds_a = {ID: test_IDs_ind[ID][:minibatchsize] for ID in test_IDs}\n",
    "test_oneshot_inds_a = np.concatenate(list(test_oneshot_inds_a.values()))\n",
    "test_oneshot_inds_b = {ID: test_IDs_ind[ID][minibatchsize:minibatchsize*2] for ID in test_IDs}\n",
    "test_oneshot_inds_b = np.concatenate(list(test_oneshot_inds_b.values()))\n",
    "test_oneshot_inds_c = {ID: test_IDs_ind[ID][minibatchsize*2:minibatchsize*3] for ID in test_IDs}\n",
    "test_oneshot_inds_c = np.concatenate(list(test_oneshot_inds_c.values()))\n",
    "test_test_inds = {ID: test_IDs_ind[ID][minibatchsize*3:] for ID in test_IDs}\n",
    "test_test_inds = np.concatenate(list(test_test_inds.values()))\n",
    "\n",
    "train_oneshot_a = big_train.iloc[train_oneshot_inds_a]\n",
    "train_oneshot_b = big_train.iloc[train_oneshot_inds_b]\n",
    "train_oneshot_c = big_train.iloc[train_oneshot_inds_c]\n",
    "train_oneshot_d = big_train.iloc[train_oneshot_inds_d]\n",
    "test_oneshot_a = small_train.iloc[test_oneshot_inds_a]\n",
    "test_oneshot_b = small_train.iloc[test_oneshot_inds_b]\n",
    "test_oneshot_c = small_train.iloc[test_oneshot_inds_c]\n",
    "test_test = small_train.iloc[test_test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:14:39.078304Z",
     "start_time": "2018-12-29T06:14:39.074394Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"shape of the 'real' testing dataset:\", test_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T06:14:52.254321Z",
     "start_time": "2018-12-29T06:14:52.154586Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../ME_data/train_oneshot_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_oneshot_a, f)\n",
    "with open(\"../ME_data/train_oneshot_b.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_oneshot_b, f)\n",
    "with open(\"../ME_data/train_oneshot_c.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_oneshot_c, f)\n",
    "with open(\"../ME_data/train_oneshot_d.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_oneshot_d, f)\n",
    "with open(\"../ME_data/test_oneshot_a.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_oneshot_a, f)\n",
    "with open(\"../ME_data/test_oneshot_b.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_oneshot_a, f)\n",
    "with open(\"../ME_data/test_oneshot_c.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_oneshot_a, f)\n",
    "with open(\"../ME_data/test_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
